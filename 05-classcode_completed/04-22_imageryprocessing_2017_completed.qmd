---
title: "Imagery processing"
format: html
---

# Introduction and goals  
On the previous exercise, we downloaded all imagery available for a field during the growing season for one year.  

Now, there are different ways we can explore these images, including:  

  - **Monitoring**: Tracking a field throughout the season, keeping an eye on the onset of stress.  
  
  - **Zoning**: Extracting pixel-based summary statistics (like median) to relate to yield potential zones.  

The goals of this exercise are to explore the two uses described above.  

# Setup  
```{r setup }
#| message: false
#| warning: false


library(tidyverse)
library(sf)
library(stars)
library(gganimate)
library(terra)
library(ggthemes)
library(mapview)
```

```{r}
boundary <- read_sf("../../05 digitizing/output/boundary.geojson") %>%
  st_transform(32616)

boundary
```

# Wrangling  
Let's import **one image** first and explore it.  

## Importing and wrangling one image  
```{r imagery}
image <- read_stars("../data/classcotton-PSScene-ortho_analytic_4b_sr-2017-105-2017-273/20170421_153046_1044_3B_AnalyticMS_SR_clip.tif")

image

```
Notice, on the second panel, we have a **band** row that we haven't seen before.  

This dimension contains all the 4 bands from this image.  

We'll need to extract the bands first. 

Let's do that first, and then calculate a vegetation index.  
I propose we calculate green normalized difference vegetation index (**gndvi**), which uses the green and the near-infrared (nir) bands.  

```{r image_w}
image_w <- image %>%
  split("band") %>%
  mutate(gndvi = (nir - green) / (nir + green) ) %>%
  dplyr::select(gndvi)

image_w
```

```{r}
mapview(image_w)
```
One down, 46 to go!  

Wait, let's find a way to automate this process.

## Importing and wrangling all images  
First, let's create a list with all tif files.  

```{r}
rastlist <- list.files(path = "../data/classcotton-PSScene-ortho_analytic_4b_sr-2017-105-2017-273/",
                       pattern = "AnalyticMS_SR_clip.tif$",
                       all.files = T,
                       full.names = T
                       )

rastlist

# Ho many images?
length(rastlist)

rastlist[[1]]
```
What information is contained within the file name?  

`20170421_153046_1044_3B_AnalyticMS_SR_clip.tif`  

**20170421** = acquisition date, in YYYYMMDD    
**153046** =  acquisition time, in HHMMSS  
**1044** = acquisition time, second hundredths  
**3B** = product level   
**AnalyticMS** = band product  
**SR** = reflectance type  
**clip** = clipped to area extent  

Now, let's create code that reads each file and applies the wrangling steps we developed above.   

We'll also need to **retrieve the date from the file name and have it as a column** in the dataframe, so we can differentiate the various images.  

> Added here time and imageid as well  

```{r allimages_w}
allimages_w <- data.frame(source = rastlist) %>%
  separate(source,
           sep = "//",
           into = c("path", "meta"),
           remove = F
           ) %>%
  mutate(date = str_sub(meta, 1, 8)) %>%
  mutate(date = as.Date(date, "%Y%m%d")) %>%
  mutate(time = str_sub(meta, 10, 15)) %>%
  mutate(time = as.numeric(time)) %>%
  dplyr::select(date, time, source) %>%
  mutate(image = map(source,
                     ~read_stars(.x)
                     )) %>%
  mutate(image_w = map(image,
                       ~ .x %>%
  split("band") %>%
  mutate(gndvi = (nir - green) / (nir + green) ) %>%
  dplyr::select(gndvi) 
                       )) %>%
  mutate(imageid = 1:nrow(.)) %>%
  dplyr::select(imageid, date, time, image_w)

allimages_w %>%
  head()

allimages_w$image_w[[1]]

allimages_w
```

```{r}
mapview(allimages_w$image_w[[4]])
```
The images were cropped to the bounding box on Planet's side.  
Let's further crop them to the field boundary.  

## Removing day duplicates  
Because the constellation has many satellites, it is possible to get more than one image per day. We need to adress it and keep only one per day to keep it simple.  


Let's check below how many dates had more than one image:  

```{r}
allimages_w %>%
  group_by(date) %>%
  tally() %>%
  filter(n > 1)
```
There were 13 dates with 2 or more images!  

Two rules to decide which image to keep:    
  - Keep the one with least NAs
  - Keep the one closest to solar noon  

Let's look into those cases:  
### 1. Least NAs  
```{r allimages_dup}
allimages_dup <- allimages_w %>%
  # Extracting number of rows and NAs
  mutate(sum = map(image_w,
                   ~.x %>%
                     as.data.frame() %>%
                     summarise(N = length(gndvi),
                               na = sum(is.na(gndvi)),
                               na_pct = round(na/N*100,0)
                               )
  )) %>%
  unnest(sum) 


allimages_dup
```

A few things to note from the above:  
  - All images have same number of observations (it is raster afterall)  
  - Normally, day duplicates have different percent of NAs (see **imageid 3 and 4**), then keep the one with least NAs.    
  - When they have duplicates with same number of NAs (see **imageid 11 and 12**), then keep the one obtained closest to solar noon.  
  - Some images may not have duplicates, yet have a large NA percentage. We can remove them as well (see **imageid 25 and 33**).    

```{r}
mapview(filter(test, imageid == 3)$image_w) +
  mapview(filter(test, imageid == 4)$image_w) 

```

### 2. Closest to solar noon  
```{r}
mapview(filter(test, imageid == 11)$image_w) +
  mapview(filter(test, imageid == 12)$image_w)
```


```{r allimages_f}
allimages_f <- allimages_dup %>%
  group_by(date) %>%
  # Keeping the one with least NAs
  filter(na == min(na)) %>%
  # Keeping the one closest to solar noon  
  filter(time == min(time)) %>%
  # Excluding images with more than 50% NAs  
  filter(na_pct < 50)

allimages_f
```

At this stage, we kept a total of **29** images (from the initial 47).  

## Cropping to field boundary  
```{r allimages_crop}
allimages_crop <- allimages_f %>%
  mutate(crop = map(image_w,
                    ~.x %>%
                      st_crop(boundary)
                    )) %>%
  dplyr::select(date, crop)

allimages_crop %>% head()

mapview(allimages_crop$crop[[4]])
```


# Pixel-based analysis  
To perform pixel-based analysis, we'll need to extract pixel-level information across all dates.  

Before we do that, I want to transform our stars (raster) into sf (vector) for ease of visualization.  

```{r pixels}
pixels <- allimages_crop %>%
  mutate(pixel = map(crop,
                     ~st_as_sf(.x, 
                               as_points = F) 
                     )) %>%
  unnest(pixel) %>%
  group_by(date) %>%
  mutate(pixid = 1:length(gndvi)) %>%
  ungroup() %>%
  st_as_sf() %>%
  dplyr::select(date, pixid, gndvi, geometry)

#pixels
```


```{r pixels}
# How many rows?
nrow(pixels)

# Peeking
head(pixels)


pixels %>%
  group_by(date) %>%
  tally()

```

## Case study: monitoring  

Let's plot all dates below.  

**WARNING**: the chunk below takes a while to run. DO NOT run in class, just look at my figure for efficiency. You create the figure on your side after class if you wish.  


```{r}
pixels %>%
  ggplot() +
  geom_sf(aes(fill = gndvi,
              color = gndvi
  ), 
  size = 1) +
  scale_fill_viridis_c(na.value = "transparent") +
  scale_color_viridis_c(na.value = "transparent") +
  theme_map() +
  theme(legend.position = "bottom") +
  facet_wrap(~date)

```


```{r}
ggsave("../output/2017_gndvi_ts.png",
       width = 6,
       height = 8, 
       bg = "white")

```


Let's take a sample of the pixels and plot them throughout the season:  

```{r}
pixels %>%
  drop_na() %>%
  filter(pixid %in% c(5500:5600)) %>%
  ggplot(aes(x = date,
             y = gndvi,
             group = pixid
             )) +
  geom_line(size = .1,
            alpha = .5
            ) +
  scale_x_date(breaks = unique(pixels$date)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

What happened on July 6th?  

Based on both plots above, let's remove low quality images based on:  
  - Cloud coverage  
  - Abnormal values  
  
## Removing low-quality images   
```{r}
pixels_f <- pixels %>%
  filter(date != "2017-07-06")

# How many rows?
nrow(pixels_f)

# Peeking

```

Now that we have a subset of high-quality images, let's extract the pixel-level **median** value across the growing season.  

## Pixel-based median  
Let's visualize what the algorithm will do for us:  
```{r}
median <- pixels_f %>%
  filter(pixid == 5500) %>%
  summarise(median = median(gndvi))

median

pixels_f %>%
  filter(pixid == 5500) %>%
  ggplot() + 
  geom_line(aes(x = date,
                y = gndvi
                )) +
  geom_hline(yintercept = median$median,
             color = "red")

```

Now let's implement it for each pixel.

If we do not drop geometry, then it takes a long time to compute the median on an sf object.  

Therefore, below we will:  
  - create a geometry back-up  
  - drop geometry 
  - calculate median, and then 
  - bring geometry back.  

```{r geometrybackup}
geometrybackup <- pixels_f %>%
  filter(date == "2017-04-21") %>%
  dplyr::select(pixid, geometry)

geometrybackup %>% head()
```

```{r pixel_median}
# Pixel-based median
pixel_median <- pixels_f %>%
  st_drop_geometry() %>%
  drop_na(gndvi) %>%
  group_by(pixid) %>%
  summarise(gndvi = median(gndvi, na.rm = T)) %>%
  left_join(geometrybackup) %>%
  st_as_sf()


pixel_median
```

Let's visualize it.  

```{r}
ggplot()+
  geom_sf(data = pixel_median, 
          aes(fill = gndvi, 
              color = gndvi
  ))+
  scale_fill_viridis_c()+
  scale_color_viridis_c()+
  labs(title = "Median gndvi")+
  theme_map()+
  theme(legend.position = "bottom")

ggsave("../output/2017_gndvi_median.png",
       width = 4,
       height = 6,
       bg = "white")


```

How does it compare to 2017 yield for this field?  
![](../../07 yield monitor/output/2017_interpolated_yield.png)

## Exporting median  
Let's export the median so we can further analyze it in the next script.  
```{r}
pixel_median %>%
  write_sf("../data/2017_pixel_median.geojson",
           delete_dsn = T
           )
  
```

